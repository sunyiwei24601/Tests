# -*- coding:UTF-8 -*-

from __future__ import print_function
import json

from pyspark.ml.classification import MultilayerPerceptronClassifier

from poseidon.util.commonUtil import CommonUtil
from poseidon.util.sparkUtil import SparkUtil

class MultilayerPerceptronClassifierCom():
    @staticmethod
    def getMultilayerPerceptronClassifier(opt):
        layersstr = opt.get('layers', None)
        layers = []
        for layer in eval(layersstr):
            layers.append(int(layer))
        mlp = MultilayerPerceptronClassifier(featuresCol="features",
                                             labelCol="label",
                                             predictionCol=opt.get('predictionCol', "prediction"),
                                             maxIter=int(opt.get('maxIter', 100)),
                                             tol=float(opt.get('tol', 1e-4)),
                                             # layers=eval(opt.get('layers', None)),
                                             layers=layers,
                                             blockSize=int(opt.get('blockSize', 128)),
                                             stepSize=float(opt.get('stepSize', 0.03)),
                                             solver=opt.get('solver', "l-bfgs")
                                             )

        seed = opt.get('seed', None)

        if seed.strip():
            mlp = mlp.setSeed(int(seed))
        else:
            mlp = mlp.setSeed(None)  # 如果用户不输入或输入空格
        return mlp

    '''
    tid:the node id
    jobj:options ie attributes of this component
    ins is a dict contains:
        in1:xxx   note:input data which can be hive table name, df type, list generated by sql
    outs is a array contains:
        out1      note:model
    '''
    @staticmethod
    def multilayerPerceptronClassifierComProcesser(tid, jobj, ins, outs, f,username,taskname):

        try:
            f.write('\n正在检查神经网络组件的参数:\n')
            f.write('tid:%s\n'%tid)
            f.write('jobj:%s\n'%json.dumps(jobj))
            f.write('ins:%s\n'%ins.__str__()[:200])
            f.write('outs:%s\n'%json.dumps(outs))

            res = {}
            training_df = None
            comOpts = jobj.get('optsEntity','')
            training_df = SparkUtil.estimatorDataProcess(tid, jobj, ins.get('in1', ''), training_df, f)
            # mlp classifier
            f.write('\n#####正在生成神经网络分类器...\n')
            mlp = MultilayerPerceptronClassifierCom.getMultilayerPerceptronClassifier(comOpts)

            f.write('\n#####正在训练神经网络分类器...\n')
            model = mlp.fit(training_df)

            #save model and model path record
            CommonUtil.saveModelNPath(username,taskname,'MultilayerPerceptronClassificationModel', model, f)
            if 'out1' in outs:
                key = '%s:%s' % (tid,'out1')
                res[key] = model
            f.write('\n#####神经网络分类器组件输出结果如下:\n%s' % res.__str__()[:200])
            return res

        except Exception as e:
            print("*****************Sorry:\n %s" % e)
            f.write("*****************Sorry:\n %s" % e)
            # f.close()
            return 0


