# -*- coding:UTF-8 -*-

from __future__ import print_function
import json

from pyspark.ml.classification import NaiveBayes

from poseidon.util.commonUtil import CommonUtil
from poseidon.util.sparkUtil import SparkUtil

class NaiveBayesCom():
    @staticmethod
    def getNaiveBayesClassifier(opt):
        nb = NaiveBayes(featuresCol="features",
                                    labelCol="label",
                                    predictionCol=opt.get('predictionCol', "prediction"),
                                    probabilityCol=opt.get('probabilityCol', "probability"),
                                    rawPredictionCol=opt.get('rawPredictionCol', "rawPrediction"),
                                    smoothing=float(opt.get('smoothing', 1.0)),
                                    modelType=opt.get('modelType', "multinomial"))
        thresholdsStr = opt.get('thresholds', None)
        if thresholdsStr and thresholdsStr != 'None' and thresholdsStr != '':
            thresholds = []
            for item in thresholdsStr.split(' '):
                thresholds.append(float(item))
            nb = nb.setThresholds(thresholds)
        return nb

    '''
    tid:the node id
    jobj:options ie attributes of this component
    ins is a dict contains:
        in1:xxx   note:input data which can be hive table name, df type, list generated by sql
    outs is a array contains:
        out1      note:model
    '''
    @staticmethod
    def naiveBayesComProcesser(tid, jobj, ins, outs, f,username,taskname):

        try:
            f.write('\n正在检查朴素贝叶斯组件的参数:\n')
            f.write('tid:%s\n'%tid)
            f.write('jobj:%s\n'%json.dumps(jobj))
            f.write('ins:%s\n'%ins.__str__()[:200])
            f.write('outs:%s\n'%json.dumps(outs))

            res = {}
            training_df = None
            comOpts = jobj.get('optsEntity','')

            #get featuresCol and labelCol from comOpts
            featuresCol = comOpts.get('featuresCol','')
            labelCol = comOpts.get('labelCol','')

            training_df = SparkUtil.estimatorDataProcess(tid, jobj, ins.get('in1',''), training_df, f)
            # rf classifier
            f.write('\n#####正在生成朴素贝叶斯分类器...\n')
            rf = NaiveBayesCom.getNaiveBayesClassifier(comOpts)

            f.write('\n#####正在训练朴素贝叶斯分类器...\n')
            model = rf.fit(training_df)

            #save model and model path record  #
            CommonUtil.saveModelNPath(username,taskname,'NaiveBayesModel', model, f)
            if 'out1' in outs:
                key = '%s:%s' % (tid,'out1')
                res[key] = model
            f.write('\n#####朴素贝叶斯分类器组件输出结果如下:\n%s' % res.__str__()[:200])
            return res

        except Exception as e:
            print("*****************Sorry:\n %s" % e)
            f.write("*****************Sorry:\n %s" % e)
            # f.close()
            return 0


