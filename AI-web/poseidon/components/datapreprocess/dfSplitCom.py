# -*- coding: utf-8 -*-
'''
split df data by the ratio specified by user
'''

from poseidon.util.sparkUtil import SparkUtil
from poseidon.util.commonUtil import CommonUtil
import xlrd
from pyspark.sql import DataFrame
from sklearn.utils import shuffle
# import random
import math

class DfSplitCom():

    def __init__(self):
        pass
    '''
    tid:the node id
    jobj:options ie attributes of this component
        ratio:training ratio specified by user
    ins is a dict contains:
        in1:xxx   note:input data which df type
    outs is a array contains:
        out1      the first piece of data which is training data which can be df type or python dict contains sql key and slist key
        out2      the second piece of data which is test data which can be df type or python dict contains sql key and slist key
    '''
    @staticmethod
    def dfSplitComProcesser(tid, jobj, ins, outs, f, *args):
        try:
            f.write('\n#####检查拆分组件的参数:\n')
            f.write('tid:%s\n'%tid)
            f.write('jobj:%s\n'%jobj.__str__())
            f.write('ins:%s\n'%ins.__str__()[:300])
            f.write('outs:%s\n'%outs.__str__()[:300])

            res = {}
            out1 = None
            out2 = None

            (conf, spark, sc) = SparkUtil.getSpark()

            comOpts = jobj.get('optsEntity') #user-defined opts
            in1 = ins.get('in1','') #input df data
            ratiostr = comOpts.get('ratio')
            seed = comOpts.get('seed', None)

            if type(in1) == type(u'str'):
                #if in1 is of str type, then we to figure out if it is of hivetable name or libsvm path
                if in1.find('hivetable:') != -1:
                    f.write('\n###########输入数据为数据表，正在将其转换为dataframe...\n')

                    in1 = spark.sql("select * from %s" % in1.split(':')[1])

                    f.write('\n###########输入数据转换后结果如下:\n %s' % in1.__str__())
                elif in1.find('libsvm:')!=-1:
                    filetype = in1.split(':')[0]
                    dfspath = in1.split(':')[1]
                    f.write('\n###########输入数据为%s，正在将其加载为dataframe...\n' % filetype)
                    if filetype == 'libsvm':
                        in1 = spark.read.format("libsvm").load(dfspath)
                        f.write('\n###########输入数据转换后结果如下:\n %s' % in1.__str__())

            if seed:
                if seed.isspace():
                    seed = None
                else:
                    seed = int(seed)
            else:
                seed = None
            training_ratio = float(ratiostr)
            test_ratio = 1-training_ratio

            if type(in1) == DataFrame:
                f.write('\n#####输入数据为dataframe格式, 切分为输出一和输出二比例为%04.2f:%04.2f\n' % (training_ratio,test_ratio))
                (out1, out2) = in1.randomSplit([training_ratio, test_ratio],seed)
            elif type(in1) == dict:
                sql = in1.get('sql','')
                #if sql, the in1 contains the result slist generated by sql of sqlCom
                if sql:
                    f.write('\n#####输入数据为包含由数组的字典,提取出数组并且切分为输出一和输出二比例%04.2f:%04.2f\n' % (training_ratio,test_ratio))
                    slist = in1.get('slist','')
                    if slist:
                        size = len(slist)
                        training_size= int(math.floor(training_ratio*size))
                        slist = shuffle(slist, random_state=seed)
                        out1 = {'sql':sql}
                        out2 = {'sql':sql}
                        out1['slist'] = slist[:training_size]
                        out2['slist'] = slist[training_size:]
            f.write('\n##### 数据拆分成功！\n')


            if 'out1' in outs:
                key = '%s:%s' % (tid,'out1')
                res[key] = out1
            if 'out2' in outs:
                key = '%s:%s' % (tid,'out2')
                res[key] = out2
            return res
        except Exception as e:
            print("*****************Sorry:\n %s" % e)
            f.write("*****************Sorry:\n %s" % e)
            # f.close()
            return 0



