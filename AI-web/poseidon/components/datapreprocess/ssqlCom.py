# -*- coding: utf-8 -*-
'''
process the hive table with spark sql
'''
from poseidon.util.sparkUtil import SparkUtil
import re
from poseidon import app

class SSQLCom():

    def __init__(self):
        pass
    '''
    tid:the node id
    jobj:options ie attributes of this component
        sql:sql will be written by user(select f1,f2 from ${t1},${t2},${t3},${t4})
    ins is a dict contains:
        in1:xxx   note:the 1th hive table or dfname
        in2:xxx   note:the 2th hive table or dfname
        in3:xxx   note:the 3th hive table or dfname
        in4:xxx   note:the 4th hive table or dfname
    outs
        out1:is name of dataframe,format:   dfname:xxx
        out2:is of dataframe ie df type
    '''
    @staticmethod
    def SSQLComProcesser(tid, jobj, ins, outs,f, username, taskname):
        try:
            f.write('\n#####正在检查SSQLCom组件的参数:\n')
            f.write('tid:%s\n'%tid)
            f.write('jobj:%s\n'%jobj.__str__())
            f.write('ins:%s\n'%ins.__str__())
            f.write('outs:%s\n'%outs.__str__())

            res = {}
            comOpts = jobj.get('optsEntity') #user-defined opts

            sql = comOpts.get('sql','') #get sql specified by user
            dfname = comOpts.get('dfname','') #dfname specified by user
            ifsave = comOpts.get('ifsave','') #if save the intermedia generated by this spark sql
            in1 = ins.get('in1','') #in1 which is table1 name or dfname
            in2 = ins.get('in2','') #in2 which is table2 name or dfname
            in3 = ins.get('in3','') #in3 which is table3 name or dfname
            in4 = ins.get('in4','') #in4 which is table4 name or dfname
            table1 = None
            table2 = None
            table3 = None
            table4 = None
            if in1:
                table1 = in1.split(':')[1]
            if in2:
                table2 = in2.split(':')[1]
            if in3:
                table3 = in3.split(':')[1]
            if in4:
                table4 = in4.split(':')[1]

            #then replace ${t1},${t2},${t3},${t4} with table1,table2,table3,table4 in sql
            sql = SSQLCom.replaceWithRelatedTableName(sql,table1,table2,table3,table4)
            f.write('\n$$$$ 替换后的sql如下:\n%s'%sql)
            f.write('\n 执行spark sql...\n')
            (conf, spark, sc) = SparkUtil.getSpark()
            print(sql)
            df = spark.sql(sql)
            #save the intermedia if the option 'ifsave' is '1'
            dbname = app.config.get('INTERMEDIA_HIVE_DB')
            if ifsave == '1':
                # df.show()
                df.write.saveAsTable('%s.%s' % (dbname, dfname), mode = 'overwrite')
            # register df to be the temporary table so that it can be handled by next SSQL com
            df.createOrReplaceTempView(dfname)

            if 'out1' in outs:
                key = '%s:%s' % (tid,'out1')
                res[key] = 'dfname:%s' % dfname
            if 'out2' in outs:
                key = '%s:%s' % (tid,'out2')
                res[key] = df
            f.write('\n#####SSQLCom组件输出如下:\n%s' % res.__str__()[:200])
            return res
        except Exception as e:
            print("*****************Sorry:\n %s" % e)
            f.write("*****************Sorry:\n %s" % e)
            # f.close()
            return 0

    @staticmethod
    def replaceWithRelatedTableName(sql,table1,table2,table3,table4):
        if table1:
            sql, number  =  re .subn('\$\{t1\}', table1, sql)
        if table2:
            sql, number  =  re .subn('\$\{t2\}', table2, sql)
        if table3:
            sql, number  =  re .subn('\$\{t3\}', table3, sql)
        if table4:
            sql, number  =  re .subn('\$\{t4\}', table4, sql)

        return sql
